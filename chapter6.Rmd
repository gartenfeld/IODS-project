# Week 6 Longitudinal Analysis

*This week we follow through the examples in Exercise Set 6.*

```{r}
date()
```

## Analysis

Read in the transformed datasets:

```{r}
library(readr)
BPRS <- read_csv("data/BPRS_long.csv", show_col_types = FALSE)
RATS <- read_csv("data/RATS_long.csv", show_col_types = FALSE)
```

### Part A

> "Implement the analyses of Chapter 8 of MABS ... but using the RATS data"

> "Look at the (column) names"

```{r}
names(RATS)
```

> "Look at the structure"

```{r}
str(RATS)
```

> "Print out summaries of the variables"

```{r}
summary(RATS)
```

> Convert categorical columns to factor

```{r}
RATS$ID <- factor(RATS$ID)
RATS$Group <- factor(RATS$Group)
```

> "Take a glimpse at the data"

```{r}
library(dplyr)
glimpse(RATS)
```

> "Draw the plot"

```{r}
library(ggplot2)
rat_count <- length(unique(RATS$ID))
colour_palette <- sample(rainbow(rat_count))
range <- c(min(RATS$Weight), max(RATS$Weight))
ggplot(RATS, aes(x = Time, y = Weight)) +
#  geom_smooth(aes(color = ID), method = "loess", se = FALSE) +
  geom_line(aes(color = ID)) +
  scale_color_manual(values = colour_palette) +
  facet_grid(. ~ Group, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = range)
```

> "Standardise the variable"

```{r}
Standardised_RATS <- RATS %>%
  group_by(Time) %>%
  mutate(Standardised_Weight = (Weight - mean(Weight))/sd(Weight)) %>%
  ungroup()
```

> "Glimpse the data"

```{r}
glimpse(Standardised_RATS)
```

> "Plot again with the standardised values"

```{r}
library(ggplot2)
rat_count <- length(unique(Standardised_RATS$ID))
colour_palette <- sample(rainbow(rat_count))
range <- c(min(Standardised_RATS$Standardised_Weight), max(Standardised_RATS$Standardised_Weight))
ggplot(Standardised_RATS, aes(x = Time, y = Standardised_Weight)) +
  geom_line(aes(color = ID)) +
  scale_color_manual(values = colour_palette) +
  facet_grid(. ~ Group, labeller = label_both) +
  theme(legend.position = "none") + 
  scale_y_continuous(limits = range, name = "Standardised Weight")
```

> Summarise data with mean and standard error of Weight by Group and Time"

```{r}
Summarised_RATS <- RATS %>%
  group_by(Group, Time) %>%
  summarise( mean = mean(Weight), se = sd(Weight) / sqrt(length(Weight)) ) %>%
  ungroup()
```

> "Glimpse the data"

```{r}
glimpse(Summarised_RATS)
```

> "Plot the mean profiles"

```{r}
ggplot(Summarised_RATS, aes(x = Time, y = mean)) +
  geom_line(aes(color = Group)) +
  geom_ribbon(aes(ymin = mean - se, ymax = mean + se, fill = Group), alpha = 0.2) +
  scale_y_continuous(name = "Weight")
```

> Check for outliers

> "Create summary data by Group and ID with mean as the summary variable

```{r}
RATS_box <- RATS %>%
  filter(Time > 0) %>%
  group_by(Group, ID) %>%
  summarise( mean=mean(Weight) ) %>%
  ungroup()
```

> "Glimpse the data"

```{r}
glimpse(RATS_box)
```

> "Draw a boxplot of the mean"

```{r}
ggplot(RATS_box, aes(x = Group, y = mean, fill = Group)) +
  geom_boxplot(alpha = 0.2, width = 0.2) +
  scale_y_continuous(name = "Mean Weight")
```

Comments: there is a fat rat in Group 2 but it's not all that extreme.

Try a filtered plot anyway:

```{r}
Group_2_filtered <- RATS_box %>%
  filter(mean < 550, Group == 2)

ggplot(Group_2_filtered, aes(y = mean, x = Group)) +
  geom_boxplot(width = 0.1) +
  scale_y_continuous(name = "Mean Weight")
```

> "Perform a two-sample t-test"

```{r}
# Since the rats fall into 3 groups, let's try testing only the first two
rats_1_2 <- subset(RATS_box, Group %in% c(1, 2))
t.test(mean ~ Group, data = rats_1_2, var.equal = TRUE)
```

This reiterates what's visible from the box plot, Group 1 & 2 are very different

Check Group 2 & 3, which are closer to each other:

```{r}
rats_2_3 <- subset(RATS_box, Group %in% c(2, 3))
t.test(mean ~ Group, data = rats_2_3, var.equal = TRUE)
```

Due to the low number of rats in each group, we can't be sure they are materially different.

Add a baseline column for the starting weight

```{r}
RATS_wide <- read_csv("data/RATS_wide.csv", show_col_types = FALSE)
RATS_based <- RATS_box %>%
  mutate(baseline = RATS_wide$WD1)
```

> "Fit a model"

```{r}
fit <- lm(mean ~ baseline + Group, data = RATS_based)
summary(fit)
```

> "Compute the ANnalysis Of VAriance table for the fitted model"

```{r}
anova(fit)
```

Here the results are a bit baffling, baseline is highly significant, meaning that the rat's starting weight explains its future weight.

Whereas, the diet is just about above the 0.05 threshold for significant, in over-generalised summary: diet doesn't matter.

For future research, perhaps the relative gains in weight could be modelled, which, I guess, was pretty much what the original Part II in the exercise set with random intercepts method was about.

#### Interpretations: Part A

Even though the empirical evidence and analysis and plots pretty much tell the story already, I will nonetheless repeat it here with words so as to satisfy the requirements of having interpretations.

Once upon a time, 16 rats lived happily in a lab where they were housed and fed. They were divided into three groups and each group had a different diet. The first group was larger than the other two, which had 8 rats, whereas the second and third group each had 4 rats. For some unknown reason, the first group of rats were all smaller, none of them weight more than 300 grams at the start. Maybe they were in an earlier life stage, maybe they came from a family of light-weighted rats, unlikely, but nobody knows. Rats in the second group all weighed over 400 grams at the start, and most rats in the third group weighed over 500 grams at the start.

The rats were put on different diets and weighed weekly over the course of 64 days, except during the 6th week they were weighed twice. Over time, practically every rat gained weight. We can see from the slopes of the lines comparing the three groups that the first group of smaller rats gained weight at a slower rate compare to the other two groups of larger rats.

It's tricky for statistical analysis when the starting point of the groups are so different, so most of the differences are explained by the baseline already. From the two-way t-test analysis, it restates the obvious that Group 1 is very different from Group 2: p = 4.059e-06. This doesn't really tell us anything new. Whereas, it's hard to say whether the diet made any difference between Group 2 and Group 3: p = 0.3101. For one thing, there are only 4 rats in each group, then, there's a really fat rat in Group 2, who weighed more than any other rat, even though rats in Group 2 were otherwise smaller than those in Group 3. So the intra-group variation is so huge, it shrouds the effect between groups.

Then, with multi-way ANOVA on the three groups and baseline, we can see that the baseline had extremely significant influence on the differences: p = 5.217e-15, whereas being in a different group and on a different diet was not even considered significant enough in the conventional sense: p = 0.07586.

### Part B

> "Implement the analyses of Chapter 9 of MABS ... but using the BPRS data"

> "Factor variables"

```{r}
BPRS$treatment <- factor(BPRS$treatment)
BPRS$subject <- factor(BPRS$subject)
```

> "Glimpse the data"

```{r}
glimpse(BPRS)
```

> "Check the dimensions of the data"

```{r}
dim(BPRS)
```

> "Plot the data"

```{r}
ggplot(BPRS, aes(x = week, y = bprs, group = subject)) +
  geom_line(aes(col = subject), alpha = 0.5) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Week") + 
  scale_y_continuous(name = "Score") +
  theme(legend.position = "none")
```

> "Create a regression model"

```{r}
BPRS_regression_model <- lm(bprs ~ week + treatment, data = BPRS)
summary(BPRS_regression_model)
```

Intercept is like the starting point, the `week` has an estimate of -2.27, meaning score decreases by 2 points per week.

Being in a different treatment group doesn't seem to do much (p = 0.661).

> "Create a random intercept model"

```{r}
library(lme4)
BPRS_RI_model <- lmer(bprs ~ week + treatment + (1 | subject), data = BPRS, REML = FALSE)
summary(BPRS_RI_model)
```

> "Create a random intercept and random slope model"

```{r}
BPRS_RI_RS_model <- lmer(bprs ~ week + treatment + (week | subject), data = BPRS, REML = FALSE)
summary(BPRS_RI_RS_model)
```

> "Perform an ANOVA test on the two models"

```{r}
anova(BPRS_RI_RS_model, BPRS_RI_model)
```

Comments: Including a random slope within subjects significantly improves the model fit compared to a model with only a random intercept.

> "Create a random intercept and random slope model with the interaction"

```{r}
BPRS_interaction <- lmer(bprs ~ week * treatment + (week | subject), data = BPRS, REML = FALSE)
summary(BPRS_interaction)
```

Compare new best to old best:

```{r}
anova(BPRS_interaction, BPRS_RI_RS_model)
```

Nah, not better enough.

> "Create a vector of the fitted values"

```{r}
Fitted <- fitted(BPRS_interaction)
```

> "Create a new column `fitted`"

```{r}
BPRS_fitted <- mutate(BPRS, Fitted = Fitted)
```

> "Draw the plot with the Fitted scores"

```{r}
library(ggplot2)
ggplot(BPRS_fitted, aes(x = week, y = Fitted, group = subject)) +
  geom_line(aes(col = subject)) +
  facet_grid(. ~ treatment, labeller = label_both) +
  scale_x_continuous(name = "Week") +
  scale_y_continuous(name = "Fitted BPRS") +
  theme(legend.position = "none")
```

So many models, it's a spaghetti plot.

#### Interpretations: Part B

In this part with the BPRS data the idea is to track the Brief Psychiatric Rating Scale scores of participant over time. 40 men were divided into two treatment groups of 20 each. Sometimes the treatment groups are divided into treatment vs. control, but in one aspect of the double-blind setup it's not revealed which group is the control. Or, gauging from the sloping trends in both groups, it's possible they both received some type of intervention. Or, it could be placebo.

Here the main idea of experimental science is to find evidence whether an intervention has an effect or not. This entails laborious work: recruiting participants, getting all the procedural things in line, collect recordings every week, and so on. All this to create the dataset of a small table. Now that part of the hard work is done, we are doing the easy part of analysing the data to test if there's an effect. This is made easier still with R and off-the-shelf packages for statistics.

What's nice about this part of the exercises is that we create multiple models, adding more complexity to account for possible underlying phenomena. Then, we can directly use ANOVA to see if the model got better. And a computer does this with lightening speed!

We can see that starting from a naïve linear regression model that the most influential factor is the passage of time. Time does heal all ills! Regardless of being in which treatment group, with each week the score decreases by about 2 points.

Then, with the mixed-effect models, we account for the hidden factors within each participant (i.e. continuity in personality) that explains the trajectory over time. The random effects show the variability in the intercepts among different subjects. The negative correlation between the intercept and time suggests that subjects with higher intercepts tend to have a steeper decline in the score. So those with higher BPRS to start with seem to benefit more from the intervention.

The ANOVA performed on the models indicated that the model with both random slope and random intercept is significantly better than the model with only random intercept: p = 0.02636. The model with added interaction is not significantly better than the model with random slope and intercept: p = 0.07495.

From the chart of the fitted lines we can visually confirm patterns described above: in both groups, participants experienced reduction of BPRS score over time. A few participants who started with very high scores got the most noticeable effects from the intervention, in both groups. The rate of change varies between individuals, ignoring the extremes, it's not instantly clear to spot the inverse correlation between intercept and slope, what we would expect to see there would be lines spaced apart, all sloping down, with wider gaps on the left and narrow gaps on the right. It is also not obvious how the treatment differs in terms of outcomes.

--- 

#### Thank you for join the journey of this course!

