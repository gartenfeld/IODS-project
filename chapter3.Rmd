# Week 3

*This week we follow through the examples in Exercise Set 3.*

```{r}
date()
```

### Step 1

Create this file and include it in `index.Rmd`.

### Step 2

> Read the joined student alcohol consumption data into R

```{r message = FALSE}
library(readr)
data <- read_csv('data/drunk_kids.csv')
```

> Print out the names of the variables in the data

```{r}
names(data)
```

> Describe the data set briefly, assuming the reader has no previous knowledge of it

The data is about high-schoolers in Portugal and their grades. The two datasets are from two subjects: maths and Portugese. G1 and G2 are from the first two periods, G3 is the final grade. The questionnaire includes a bunch of socio-economic, behavioural, and other factors. In this exercise we wonder if binge drinking kids have worse study outcomes.

> The purpose of your analysis is to study the relationships between high/low alcohol consumption and some of the other variables in the data. To do this, choose 4 interesting variables in the data and for each of them, present your personal hypothesis about their relationships with alcohol consumption.

Choices of variables and reasoning:

-   `sex`: interesting to see if there's a gender difference in consumption
-   `romantic`: cool kids drink more often and socialise more
-   `famrel`: kids with bad family relationship drink more
-   `goout`: kids go out with friends and drink

> Numerically and graphically explore the distributions of your chosen variables and their relationships with alcohol consumption (use for example cross-tabulations, bar plots and box plots). Comment on your findings and compare the results of your exploration to your previously stated hypotheses.

```{r message = FALSE}
library(dplyr)
data %>% count(sex)
```

```{r}
library(ggplot2)

data_for_plot <- tidyr::gather(data, key = "timing", value = "combined_use", Dalc, Walc)


ggplot(data_for_plot, aes(x = sex, y = combined_use, fill = timing)) +
  geom_boxplot(position = "dodge", alpha = 0.2) +

  labs(
       x = "Sex",
       y = "Alcohol Consumption (5 = Very high)",
       fill = "Occasion",
  ) +

  scale_fill_manual(
    values = c("Dalc" = "lightblue", "Walc" = "lightgreen"),
    labels = c("Dalc" = "Weekday", "Walc" = "Weekend")
  )
```

------------------------------------------------------------------------

Comments:

-   Boys drink more
-   Girls rarely drink during the week
-   There may be a significant difference between the sexes

------------------------------------------------------------------------

```{r message = FALSE}
library(ggplot2)

ggplot(data, aes(x = romantic, y = alc_use, fill = romantic)) +
  geom_boxplot(position = position_dodge(0.8), alpha = 0.8) +
  labs(title = "Alcohol Consumption by Sex and Romantic Relationship",
       x = "Sex",
       y = "Alcohol Consumption",
       fill = "Romantic") +
  scale_fill_manual(values = c("yes" = "pink", "no" = "lightgrey"),
                    name = "In Relationship",
                    labels = c("yes" = "Yes", "no" = "No")) +
  facet_grid(. ~ sex)

```

------------------------------------------------------------------------

Comments:

-   Alcohol makes no difference in girls
-   It also makes almost no difference in boys
-   Therefore one cannot drink one's way to pair bonding

------------------------------------------------------------------------

```{r message = FALSE}
library(ggplot2)

ggplot(data, aes(x = famrel, y = alc_use, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Alcohol Consumption and Family Relationship",
       x = "Family Relationship",
       y = "Alcohol Consumption",
       color = "Sex") +
  scale_color_manual(values = c("M" = "deepskyblue", "F" = "pink"),
                     name = "Sex")
```

------------------------------------------------------------------------

Comments:

-   For boys, the trend seems more noticeable, that one drinks less in a good family
-   For girls the line seems pretty flat, that all family environments have relatively low drinking

------------------------------------------------------------------------

```{r message = FALSE}
library(ggplot2)

ggplot(data, aes(x = goout, y = alc_use, color = sex)) +
  geom_point() +
  geom_smooth(method = "lm") +
  labs(title = "Alcohol Consumption and Going Out",
       x = "Going Out with Friends",
       y = "Alcohol Consumption",
       color = "Sex") +
  scale_color_manual(values = c("M" = "deepskyblue", "F" = "pink"),
                     name = "Sex")
```

------------------------------------------------------------------------

Comments:

-   Quite obvious trends of going out involving drinking

------------------------------------------------------------------------

### Step 5

> Use logistic regression to statistically explore the relationship between your chosen variables and the binary high/low alcohol consumption variable as the target variable.

```{r message = FALSE}
alco_predictor <- glm(high_use ~ famrel + goout, data = data, family = "binomial")
```

> Present and interpret a summary of the fitted model.

```{r message = FALSE}
summary(alco_predictor)
```

Interpretation:

-   Family relationship goodness has a significant negative influence on high use
-   Going out with friends has a significant positive influence on high use

> Present and interpret the coefficients of the model as odds ratios and provide confidence intervals for them. Interpret the results and compare them to your previously stated hypothesis.

```{r message = FALSE}
coef(alco_predictor)
```

Interpretation:

-   An increase in `goout` is associated with an increase in the log-odds of `high_use`, while an increase in `famrel` is associated with a decrease in the log-odds of `high_use`.

### Step 6

> Using the variables which, according to your logistic regression model, had a statistical relationship with high/low alcohol consumption, explore the predictive power of you model. Provide a 2x2 cross tabulation of predictions versus the actual values and optionally display a graphic visualizing both the actual values and the predictions.

```{r message = FALSE}
alco_predictor <- glm(high_use ~ famrel + goout, data = data, family = "binomial")
probs <- predict(alco_predictor, type = "response")

library(dplyr)
forwards_data <- mutate(data, probability = probs)
forwards_data <- mutate(forwards_data, prediction = probability > 0.5)

table(high_use = forwards_data$high_use, prediction = forwards_data$prediction)
```
> Compute the total proportion of inaccurately classified individuals (= the training error) and comment on all the results.

```{r message = FALSE}
TP <- 47
TN <- 233
FP <- 26
FN <- 64
training_error <- (FP + FN) / (TP + TN + FP + FN)
training_error
```

> Compare the performance of the model with performance achieved by some simple guessing strategy.

So it gets a quarter of the cases wrong. Personally, I'm not sure what is a "simple guessing strategy", it's better than a coin toss, but given the input of family relationship and going out, plus some human intuition, guessing could have fared better or worse. But 75+% correct classification seems to be a rather good first-try result in machine learning learning land.

### Step 7

> Perform 10-fold cross-validation on your model.


```{r message = FALSE}
data$high_use <- as.factor(data$high_use)
alco_predictor <- glm(high_use ~ famrel + goout, data = data, family = "binomial")

# Run `install.packages('caret', dependencies = TRUE)` beforehand
library(caret)
# Create a training control object for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Perform cross-validation using caret
cv_results <- train(high_use ~ famrel + goout, data = data, method = "glm", family = "binomial", trControl = ctrl)

# View cross-validation results
cv_results
```
The prediction is 74% accurate and quite a bit better than chance.

The performance of this model is on par with the "model introduced in the Exercise Set (which had about 0.26 error)".


### Step 8

> Perform cross-validation to compare the performance of different logistic regression models (= different sets of predictors). Start with a very high number of predictors and explore the changes in the training and testing errors as you move to models with less predictors. Draw a graph displaying the trends of both training and testing errors by the number of predictors in the model.

Noniin, let's start with a "very high number of predictors".

```{r message = FALSE}
data$high_use <- as.factor(data$high_use)
alco_predictor <- glm(high_use ~ address + famsize + Pstatus + Medu + Fedu + Mjob + Fjob + reason + guardian + traveltime + studytime + schoolsup + famsup + activities + nursery + higher + internet + romantic + freetime + health + failures + paid + absences + G1 + G2 + G3 + famrel + goout, data = data, family = "binomial")

# Run `install.packages('caret', dependencies = TRUE)` beforehand
library(caret)
# Create a training control object for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Perform cross-validation using caret
cv_results <- train(high_use ~ address + famsize + Pstatus + Medu + Fedu + Mjob + Fjob + reason + guardian + traveltime + studytime + schoolsup + famsup + activities + nursery + higher + internet + romantic + freetime + health + failures + paid + absences + G1 + G2 + G3 + famrel + goout, data = data, method = "glm", family = "binomial", trControl = ctrl)

# View cross-validation results
cv_results
```
Not much better...

Now, with hand-picked variables:

```{r message = FALSE}
data$high_use <- as.factor(data$high_use)
alco_predictor <- glm(high_use ~ address + Pstatus + higher + absences + famrel + goout, data = data, family = "binomial")

# Run `install.packages('caret', dependencies = TRUE)` beforehand
library(caret)
# Create a training control object for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Perform cross-validation using caret
cv_results <- train(high_use ~ address + Pstatus + higher + absences + famrel + goout, data = data, method = "glm", family = "binomial", trControl = ctrl)

# View cross-validation results
cv_results
```

After manually trying a bunch of combos, it doesn't seem to get much better than this... due to the random round-robin cross-validation, the value also jumps up and down making it hard to eyeball whether it's better or worse.

But wait, what about...

```{r warning = FALSE}
data$high_use <- as.factor(data$high_use)
alco_predictor <- glm(high_use ~ Walc + Dalc, data = data, family = "binomial")

# Run `install.packages('caret', dependencies = TRUE)` beforehand
library(caret)
# Create a training control object for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# Perform cross-validation using caret
cv_results <- train(high_use ~ Walc + Dalc, data = data, method = "glm", family = "binomial", trControl = ctrl)

# View cross-validation results
cv_results
```

Well, statistical truths are often tautological.
